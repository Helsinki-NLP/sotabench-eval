{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to sotabencheval! You have reached the docs for the sotabencheval library. This library contains a collection of deep learning benchmarks you can use to benchmark your models. It can be used in conjunction with the sotabench.com website to record results for models, so the community can compare model performance on different tasks, as well as a continuous integration style service for your repository to benchmark your models on each commit. sotabencheval is a general benchmarking library, meaning it is designed to support all deep learning frameworks, and requires minimal code integration. There are alternative sotabench APIs you can use that are specialized for particular frameworks, e.g. torchbench for PyTorch. Getting Started : Benchmarking on ImageNet Step One : Create a sotabench.py file in the root of your repository This can contain whatever logic you need to load and process the dataset, and to produce model predictions for it. To record your results for sotabench, initialise an ImageNet evaluator object to name the model (and optionally) link to a paper: from sotabencheval.image_classification import ImageNetEvaluator evaluator = ImageNetEvaluator ( model_name = 'ResNeXt-101-32x8d' , paper_arxiv_id = '1611.05431' ) For each batch of predictions made by your model, pass a dictionary of keys as image IDs and values as output predictions to the evaluator.add method: evaluator . add ( dict ( zip ( image_ids , batch_output ))) Then after you have accumulated all the predictions: evaluator . save () This will ensure results are evaluated and saved when they are run on the sotabench server. Below you can see a working sotabench.py file added to the torchvision repository to test one of its models, integrating the evaluation code from above: import numpy as np import PIL import torch from torch.utils.data import DataLoader from torchvision.models.resnet import resnext101_32x8d import torchvision.transforms as transforms from torchvision.datasets import ImageNet from sotabencheval.image_classification import ImageNetEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/imagenet' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' model = resnext101_32x8d ( pretrained = True ) input_transform = transforms . Compose ([ transforms . Resize ( 256 , PIL . Image . BICUBIC ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]), ]) test_dataset = ImageNet ( DATA_ROOT , split = \"val\" , transform = input_transform , target_transform = None , download = True , ) test_loader = DataLoader ( test_dataset , batch_size = 128 , shuffle = False , num_workers = 4 , pin_memory = True , ) model = model . cuda () model . eval () evaluator = ImageNetEvaluator ( model_name = 'ResNeXt-101-32x8d' , paper_arxiv_id = '1611.05431' ) def get_img_id ( image_name ): return image_name . split ( '/' )[ - 1 ] . replace ( '.JPEG' , '' ) with torch . no_grad (): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) evaluator . save () Step Two : Run locally to verify that it works python sotabench.py You can also run the logic in a Jupyter Notebook if that is your preferred workflow. Step Three : Login and connect your repository to sotabench Create an account on sotabench , then head to your user page. Click the Connect a GitHub repository button: Then follow the steps to connect the repositories that you wish to benchmark: After you connect your repository, the sotabench servers will re-evaluate your model on every commit, to ensure the model is working and results are up-to-date - including if you add additional models to the benchmark file. Installation The library requires Python 3.6+. You can install via pip: pip install sotabencheval Support If you get stuck you can head to our Discourse forum where you ask questions on how to use the project. You can also find ideas for contributions, and work with others on exciting projects.","title":"Welcome to sotabencheval!"},{"location":"#welcome-to-sotabencheval","text":"You have reached the docs for the sotabencheval library. This library contains a collection of deep learning benchmarks you can use to benchmark your models. It can be used in conjunction with the sotabench.com website to record results for models, so the community can compare model performance on different tasks, as well as a continuous integration style service for your repository to benchmark your models on each commit. sotabencheval is a general benchmarking library, meaning it is designed to support all deep learning frameworks, and requires minimal code integration. There are alternative sotabench APIs you can use that are specialized for particular frameworks, e.g. torchbench for PyTorch.","title":"Welcome to sotabencheval!"},{"location":"#getting-started-benchmarking-on-imagenet","text":"Step One : Create a sotabench.py file in the root of your repository This can contain whatever logic you need to load and process the dataset, and to produce model predictions for it. To record your results for sotabench, initialise an ImageNet evaluator object to name the model (and optionally) link to a paper: from sotabencheval.image_classification import ImageNetEvaluator evaluator = ImageNetEvaluator ( model_name = 'ResNeXt-101-32x8d' , paper_arxiv_id = '1611.05431' ) For each batch of predictions made by your model, pass a dictionary of keys as image IDs and values as output predictions to the evaluator.add method: evaluator . add ( dict ( zip ( image_ids , batch_output ))) Then after you have accumulated all the predictions: evaluator . save () This will ensure results are evaluated and saved when they are run on the sotabench server. Below you can see a working sotabench.py file added to the torchvision repository to test one of its models, integrating the evaluation code from above: import numpy as np import PIL import torch from torch.utils.data import DataLoader from torchvision.models.resnet import resnext101_32x8d import torchvision.transforms as transforms from torchvision.datasets import ImageNet from sotabencheval.image_classification import ImageNetEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/imagenet' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' model = resnext101_32x8d ( pretrained = True ) input_transform = transforms . Compose ([ transforms . Resize ( 256 , PIL . Image . BICUBIC ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]), ]) test_dataset = ImageNet ( DATA_ROOT , split = \"val\" , transform = input_transform , target_transform = None , download = True , ) test_loader = DataLoader ( test_dataset , batch_size = 128 , shuffle = False , num_workers = 4 , pin_memory = True , ) model = model . cuda () model . eval () evaluator = ImageNetEvaluator ( model_name = 'ResNeXt-101-32x8d' , paper_arxiv_id = '1611.05431' ) def get_img_id ( image_name ): return image_name . split ( '/' )[ - 1 ] . replace ( '.JPEG' , '' ) with torch . no_grad (): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) evaluator . save () Step Two : Run locally to verify that it works python sotabench.py You can also run the logic in a Jupyter Notebook if that is your preferred workflow. Step Three : Login and connect your repository to sotabench Create an account on sotabench , then head to your user page. Click the Connect a GitHub repository button: Then follow the steps to connect the repositories that you wish to benchmark: After you connect your repository, the sotabench servers will re-evaluate your model on every commit, to ensure the model is working and results are up-to-date - including if you add additional models to the benchmark file.","title":"Getting Started : Benchmarking on ImageNet"},{"location":"#installation","text":"The library requires Python 3.6+. You can install via pip: pip install sotabencheval","title":"Installation"},{"location":"#support","text":"If you get stuck you can head to our Discourse forum where you ask questions on how to use the project. You can also find ideas for contributions, and work with others on exciting projects.","title":"Support"},{"location":"ade20k/","text":"ADE20K You can view the ADE20K leaderboard here . Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the ADE20K 2012 dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server ADE20K data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. Server Data Location The ADE20K data is located in the root of your repository on the server at .data/vision/ade20k . In this folder is contained: ADEChallengeData2016.zip - containing validation images and annotations Your local ADE20K files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/ade20k' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly. How Do I Initialize an Evaluator? Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.semantic_segmentation import ADE20KEvaluator evaluator = ADE20KEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper. For example: from sotabencheval.semantic_segmentation import ADE20KEvaluator evaluator = ADE20KEvaluator ( model_name = 'OCR (HRNetV2-W48)' , paper_arxiv_id = '1909.11065' ) The above will directly compare with the result of the paper when run on the server. How Do I Evaluate Predictions? The evaluator object has an .add() method to submit predictions by batch or in full. For ADE20K there are two required arguments: outputs , a 1D np.ndarray of semantic class predictions per label, and targets , a 1D np.ndarray of ground truth semantic classes per pixel. In other words, it requires flattened inputs and outputs. To elaborate, suppose you are making predictions, batch by batch, and have your model output and the original targets with batch_size 32 , and image size (520, 480) . The shape of your outputs might look like: batch_output . shape >> ( 32 , 150 , 520 , 480 ) # where 150 is the number of ADE20K classes batch_target . shape >> ( 32 , 520 , 480 ) We can flatten the entire output and targets to 1D vectors for each pixel: flattened_batch_output . shape >> ( 7987200 ) # flatten by taking the max class prediction # (batch_output.argmax(1).flatten() in torch with class as second dimension) flattened_batch_target . shape >> ( 7987200 ) # (batch_target.flatten() in torch) The output might look something like this: flattened_batch_output >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) flattened_batch_target >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) In both cases, the prediction and ground truth have class 6 as the semantic label for the first 5 pixels - so the model is correct. These flattened arrays can then be passed into the .add() method of the evaluator my_evaluator . update ( outputs = flattened_batch_output , targets = flattened_batch_target ) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would like something like this (for a PyTorch example): evaluator = ADE20KEvaluator ( model_name = 'OCR (HRNetV2-W48)' , paper_arxiv_id = '1909.11065' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): evaluator = ADE20KEvaluator ( model_name = 'OCR (HRNetV2-W48)' , paper_arxiv_id = '1909.11065' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. Need More Help? Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"ADE20K"},{"location":"ade20k/#ade20k","text":"You can view the ADE20K leaderboard here .","title":"ADE20K"},{"location":"ade20k/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the ADE20K 2012 dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server ADE20K data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"ade20k/#server-data-location","text":"The ADE20K data is located in the root of your repository on the server at .data/vision/ade20k . In this folder is contained: ADEChallengeData2016.zip - containing validation images and annotations Your local ADE20K files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/ade20k' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly.","title":"Server Data Location"},{"location":"ade20k/#how-do-i-initialize-an-evaluator","text":"Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.semantic_segmentation import ADE20KEvaluator evaluator = ADE20KEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper. For example: from sotabencheval.semantic_segmentation import ADE20KEvaluator evaluator = ADE20KEvaluator ( model_name = 'OCR (HRNetV2-W48)' , paper_arxiv_id = '1909.11065' ) The above will directly compare with the result of the paper when run on the server.","title":"How Do I Initialize an Evaluator?"},{"location":"ade20k/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add() method to submit predictions by batch or in full. For ADE20K there are two required arguments: outputs , a 1D np.ndarray of semantic class predictions per label, and targets , a 1D np.ndarray of ground truth semantic classes per pixel. In other words, it requires flattened inputs and outputs. To elaborate, suppose you are making predictions, batch by batch, and have your model output and the original targets with batch_size 32 , and image size (520, 480) . The shape of your outputs might look like: batch_output . shape >> ( 32 , 150 , 520 , 480 ) # where 150 is the number of ADE20K classes batch_target . shape >> ( 32 , 520 , 480 ) We can flatten the entire output and targets to 1D vectors for each pixel: flattened_batch_output . shape >> ( 7987200 ) # flatten by taking the max class prediction # (batch_output.argmax(1).flatten() in torch with class as second dimension) flattened_batch_target . shape >> ( 7987200 ) # (batch_target.flatten() in torch) The output might look something like this: flattened_batch_output >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) flattened_batch_target >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) In both cases, the prediction and ground truth have class 6 as the semantic label for the first 5 pixels - so the model is correct. These flattened arrays can then be passed into the .add() method of the evaluator my_evaluator . update ( outputs = flattened_batch_output , targets = flattened_batch_target ) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would like something like this (for a PyTorch example): evaluator = ADE20KEvaluator ( model_name = 'OCR (HRNetV2-W48)' , paper_arxiv_id = '1909.11065' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"ade20k/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): evaluator = ADE20KEvaluator ( model_name = 'OCR (HRNetV2-W48)' , paper_arxiv_id = '1909.11065' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"ade20k/#need-more-help","text":"Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"},{"location":"coco/","text":"COCO You can view the COCO minival leaderboard here . Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the COCO dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server COCO data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. Server Data Location The COCO validation data is located in the root of your repository on the server at .data/vision/coco . In this folder is contained: annotations_trainval2017.zip - containing annotations for the validation images val2017.zip - containing the validation images Your local COCO files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/coco' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly. How Do I Initialize an Evaluator? Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.object_detection import COCOEvaluator evaluator = COCOEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper's model. For example: from sotabencheval.object_detection import COCOEvaluator evaluator = COCOEvaluator ( model_name = 'Mask R-CNN' , paper_arxiv_id = '1703.06870' ) The above will directly compare with the result of the paper when run on the server. How Do I Evaluate Predictions? The evaluator object has an .add() method to submit predictions by batch or in full. For COCO the expected input is a list of dictionaries, where each dictionary contains detection information that will be used by the loadRes method based on the pycocotools API. Each detection can take a dictionary like the following: { 'image_id' : 397133 , 'bbox' : [ 386.1628112792969 , 69.48855590820312 , 110.14895629882812 , 278.2847595214844 ], 'score' : 0.999152421951294 , 'category_id' : 1 } For this benchmark, only bounding box detection ('bbox') is performed at present. You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would look something like this (for a PyTorch example): ... evaluator = COCOEvaluator ( model_name = 'Mask R-CNN' , paper_arxiv_id = '1703.06870' ) with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. A Full sotabench.py Example Below we show an implementation for a model from the torchvision repository. This incorporates all the features explained above: (a) using the server data root, (b) using the COCO Evaluator, and (c) caching the evaluation logic. Note that the torchbench dependency is just to get some processing logic and transforms; the evaluation is done with sotabencheval. import os import tqdm import torch from torch.utils.data import DataLoader from torchbench.utils import send_model_to_device from torchbench.object_detection.transforms import Compose , ConvertCocoPolysToMask , ToTensor import torchvision import PIL from sotabencheval.object_detection import COCOEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/coco' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' def coco_data_to_device ( input , target , device : str = \"cuda\" , non_blocking : bool = True ): input = list ( inp . to ( device = device , non_blocking = non_blocking ) for inp in input ) target = [{ k : v . to ( device = device , non_blocking = non_blocking ) for k , v in t . items ()} for t in target ] return input , target def coco_collate_fn ( batch ): return tuple ( zip ( * batch )) def coco_output_transform ( output , target ): output = [{ k : v . to ( \"cpu\" ) for k , v in t . items ()} for t in output ] return output , target transforms = Compose ([ ConvertCocoPolysToMask (), ToTensor ()]) model = torchvision . models . detection . __dict__ [ 'maskrcnn_resnet50_fpn' ]( num_classes = 91 , pretrained = True ) model , device = send_model_to_device ( model , device = 'cuda' , num_gpu = 1 ) model . eval () model_output_transform = coco_output_transform send_data_to_device = coco_data_to_device collate_fn = coco_collate_fn test_dataset = torchbench . datasets . CocoDetection ( root = os . path . join ( DATA_ROOT , \"val %s \" % '2017' ), annFile = os . path . join ( DATA_ROOT , \"annotations/instances_val %s .json\" % '2017' ), transform = None , target_transform = None , transforms = transforms , download = True , ) test_loader = DataLoader ( test_dataset , batch_size = 8 , shuffle = False , num_workers = 4 , pin_memory = True , collate_fn = collate_fn , ) test_loader . no_classes = 91 # Number of classes for COCO Detection iterator = tqdm . tqdm ( test_loader , desc = \"Evaluation\" , mininterval = 5 ) evaluator = COCOEvaluator ( root = DATA_ROOT , model_name = 'Mask R-CNN (ResNet-50-FPN)' , paper_arxiv_id = '1703.06870' def prepare_for_coco_detection ( predictions ): coco_results = [] for original_id , prediction in predictions . items (): if len ( prediction ) == 0 : continue boxes = prediction [ \"boxes\" ] boxes = convert_to_xywh ( boxes ) . tolist () scores = prediction [ \"scores\" ] . tolist () labels = prediction [ \"labels\" ] . tolist () coco_results . extend ( [ { \"image_id\" : original_id , \"category_id\" : labels [ k ], \"bbox\" : box , \"score\" : scores [ k ], } for k , box in enumerate ( boxes ) ] ) return coco_results def convert_to_xywh ( boxes ): xmin , ymin , xmax , ymax = boxes . unbind ( 1 ) return torch . stack (( xmin , ymin , xmax - xmin , ymax - ymin ), dim = 1 ) with torch . no_grad (): for i , ( input , target ) in enumerate ( iterator ): input , target = send_data_to_device ( input , target , device = device ) original_output = model ( input ) output , target = model_output_transform ( original_output , target ) result = { tar [ \"image_id\" ] . item (): out for tar , out in zip ( target , output ) } result = prepare_for_coco_detection ( result ) evaluator . update ( result ) if evaluator . cache_exists : break evaluator . save () Need More Help? Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"COCO"},{"location":"coco/#coco","text":"You can view the COCO minival leaderboard here .","title":"COCO"},{"location":"coco/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the COCO dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server COCO data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"coco/#server-data-location","text":"The COCO validation data is located in the root of your repository on the server at .data/vision/coco . In this folder is contained: annotations_trainval2017.zip - containing annotations for the validation images val2017.zip - containing the validation images Your local COCO files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/coco' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly.","title":"Server Data Location"},{"location":"coco/#how-do-i-initialize-an-evaluator","text":"Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.object_detection import COCOEvaluator evaluator = COCOEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper's model. For example: from sotabencheval.object_detection import COCOEvaluator evaluator = COCOEvaluator ( model_name = 'Mask R-CNN' , paper_arxiv_id = '1703.06870' ) The above will directly compare with the result of the paper when run on the server.","title":"How Do I Initialize an Evaluator?"},{"location":"coco/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add() method to submit predictions by batch or in full. For COCO the expected input is a list of dictionaries, where each dictionary contains detection information that will be used by the loadRes method based on the pycocotools API. Each detection can take a dictionary like the following: { 'image_id' : 397133 , 'bbox' : [ 386.1628112792969 , 69.48855590820312 , 110.14895629882812 , 278.2847595214844 ], 'score' : 0.999152421951294 , 'category_id' : 1 } For this benchmark, only bounding box detection ('bbox') is performed at present. You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would look something like this (for a PyTorch example): ... evaluator = COCOEvaluator ( model_name = 'Mask R-CNN' , paper_arxiv_id = '1703.06870' ) with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"coco/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"coco/#a-full-sotabenchpy-example","text":"Below we show an implementation for a model from the torchvision repository. This incorporates all the features explained above: (a) using the server data root, (b) using the COCO Evaluator, and (c) caching the evaluation logic. Note that the torchbench dependency is just to get some processing logic and transforms; the evaluation is done with sotabencheval. import os import tqdm import torch from torch.utils.data import DataLoader from torchbench.utils import send_model_to_device from torchbench.object_detection.transforms import Compose , ConvertCocoPolysToMask , ToTensor import torchvision import PIL from sotabencheval.object_detection import COCOEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/coco' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' def coco_data_to_device ( input , target , device : str = \"cuda\" , non_blocking : bool = True ): input = list ( inp . to ( device = device , non_blocking = non_blocking ) for inp in input ) target = [{ k : v . to ( device = device , non_blocking = non_blocking ) for k , v in t . items ()} for t in target ] return input , target def coco_collate_fn ( batch ): return tuple ( zip ( * batch )) def coco_output_transform ( output , target ): output = [{ k : v . to ( \"cpu\" ) for k , v in t . items ()} for t in output ] return output , target transforms = Compose ([ ConvertCocoPolysToMask (), ToTensor ()]) model = torchvision . models . detection . __dict__ [ 'maskrcnn_resnet50_fpn' ]( num_classes = 91 , pretrained = True ) model , device = send_model_to_device ( model , device = 'cuda' , num_gpu = 1 ) model . eval () model_output_transform = coco_output_transform send_data_to_device = coco_data_to_device collate_fn = coco_collate_fn test_dataset = torchbench . datasets . CocoDetection ( root = os . path . join ( DATA_ROOT , \"val %s \" % '2017' ), annFile = os . path . join ( DATA_ROOT , \"annotations/instances_val %s .json\" % '2017' ), transform = None , target_transform = None , transforms = transforms , download = True , ) test_loader = DataLoader ( test_dataset , batch_size = 8 , shuffle = False , num_workers = 4 , pin_memory = True , collate_fn = collate_fn , ) test_loader . no_classes = 91 # Number of classes for COCO Detection iterator = tqdm . tqdm ( test_loader , desc = \"Evaluation\" , mininterval = 5 ) evaluator = COCOEvaluator ( root = DATA_ROOT , model_name = 'Mask R-CNN (ResNet-50-FPN)' , paper_arxiv_id = '1703.06870' def prepare_for_coco_detection ( predictions ): coco_results = [] for original_id , prediction in predictions . items (): if len ( prediction ) == 0 : continue boxes = prediction [ \"boxes\" ] boxes = convert_to_xywh ( boxes ) . tolist () scores = prediction [ \"scores\" ] . tolist () labels = prediction [ \"labels\" ] . tolist () coco_results . extend ( [ { \"image_id\" : original_id , \"category_id\" : labels [ k ], \"bbox\" : box , \"score\" : scores [ k ], } for k , box in enumerate ( boxes ) ] ) return coco_results def convert_to_xywh ( boxes ): xmin , ymin , xmax , ymax = boxes . unbind ( 1 ) return torch . stack (( xmin , ymin , xmax - xmin , ymax - ymin ), dim = 1 ) with torch . no_grad (): for i , ( input , target ) in enumerate ( iterator ): input , target = send_data_to_device ( input , target , device = device ) original_output = model ( input ) output , target = model_output_transform ( original_output , target ) result = { tar [ \"image_id\" ] . item (): out for tar , out in zip ( target , output ) } result = prepare_for_coco_detection ( result ) evaluator . update ( result ) if evaluator . cache_exists : break evaluator . save ()","title":"A Full sotabench.py Example"},{"location":"coco/#need-more-help","text":"Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"},{"location":"imagenet/","text":"ImageNet You can view the ImageNet leaderboard here . Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the ImageNet dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server ImageNet data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. Server Data Location The ImageNet validation data is located in the root of your repository on the server at .data/vision/imagenet . In this folder is contained: ILSVRC2012_devkit_t12.tar.gz - containing metadata ILSVRC2012_img_val.tar - containing the validation images Your local ImageNet files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/imagenet' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly. How Do I Initialize an Evaluator? Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.image_classification import ImageNetEvaluator evaluator = ImageNetEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper's model. For example: from sotabencheval.image_classification import ImageNetEvaluator evaluator = ImageNetEvaluator ( model_name = 'FixResNeXt-101 32x48d' , paper_arxiv_id = '1906.06423' ) The above will directly compare with the result of the paper when run on the server. How Do I Evaluate Predictions? The evaluator object has an .add() method to submit predictions by batch or in full. For ImageNet the expected input as a dictionary of outputs, where each key is an image ID from ImageNet and each value is a list or 1D numpy array of logits for that image ID. For example: evaluator . add ({ 'ILSVRC2012_val_00000293' : np . array ([ 1.04243 , ... ]), 'ILSVRC2012_val_00000294' : np . array ([ - 2.3677 , ... ])}) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would like something like this (for a PyTorch example): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input ) image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input ) image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. A full sotabench.py example Below we show an implementation for a model from the torchvision repository. This incorporates all the features explained above: (a) using the server data root, (b) using the ImageNet Evaluator, and (c) caching the evaluation logic: import numpy as np import PIL import torch from torchvision.models.resnet import resnext101_32x8d import torchvision.transforms as transforms from torchvision.datasets import ImageNet from torch.utils.data import DataLoader from sotabencheval.image_classification import ImageNetEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/imagenet' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' model = resnext101_32x8d ( pretrained = True ) input_transform = transforms . Compose ([ transforms . Resize ( 256 , PIL . Image . BICUBIC ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]), ]) test_dataset = ImageNet ( DATA_ROOT , split = \"val\" , transform = input_transform , target_transform = None , download = True , ) test_loader = DataLoader ( test_dataset , batch_size = 128 , shuffle = False , num_workers = 4 , pin_memory = True , ) model = model . cuda () model . eval () evaluator = ImageNetEvaluator ( model_name = 'ResNeXt-101-32x8d' , paper_arxiv_id = '1611.05431' ) def get_img_id ( image_name ): return image_name . split ( '/' )[ - 1 ] . replace ( '.JPEG' , '' ) with torch . no_grad (): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input ) image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) if evaluator . cache_exists : break evaluator . save () Need More Help? Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"ImageNet"},{"location":"imagenet/#imagenet","text":"You can view the ImageNet leaderboard here .","title":"ImageNet"},{"location":"imagenet/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the ImageNet dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server ImageNet data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"imagenet/#server-data-location","text":"The ImageNet validation data is located in the root of your repository on the server at .data/vision/imagenet . In this folder is contained: ILSVRC2012_devkit_t12.tar.gz - containing metadata ILSVRC2012_img_val.tar - containing the validation images Your local ImageNet files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/imagenet' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly.","title":"Server Data Location"},{"location":"imagenet/#how-do-i-initialize-an-evaluator","text":"Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.image_classification import ImageNetEvaluator evaluator = ImageNetEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper's model. For example: from sotabencheval.image_classification import ImageNetEvaluator evaluator = ImageNetEvaluator ( model_name = 'FixResNeXt-101 32x48d' , paper_arxiv_id = '1906.06423' ) The above will directly compare with the result of the paper when run on the server.","title":"How Do I Initialize an Evaluator?"},{"location":"imagenet/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add() method to submit predictions by batch or in full. For ImageNet the expected input as a dictionary of outputs, where each key is an image ID from ImageNet and each value is a list or 1D numpy array of logits for that image ID. For example: evaluator . add ({ 'ILSVRC2012_val_00000293' : np . array ([ 1.04243 , ... ]), 'ILSVRC2012_val_00000294' : np . array ([ - 2.3677 , ... ])}) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would like something like this (for a PyTorch example): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input ) image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"imagenet/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input ) image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"imagenet/#a-full-sotabenchpy-example","text":"Below we show an implementation for a model from the torchvision repository. This incorporates all the features explained above: (a) using the server data root, (b) using the ImageNet Evaluator, and (c) caching the evaluation logic: import numpy as np import PIL import torch from torchvision.models.resnet import resnext101_32x8d import torchvision.transforms as transforms from torchvision.datasets import ImageNet from torch.utils.data import DataLoader from sotabencheval.image_classification import ImageNetEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/imagenet' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' model = resnext101_32x8d ( pretrained = True ) input_transform = transforms . Compose ([ transforms . Resize ( 256 , PIL . Image . BICUBIC ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]), ]) test_dataset = ImageNet ( DATA_ROOT , split = \"val\" , transform = input_transform , target_transform = None , download = True , ) test_loader = DataLoader ( test_dataset , batch_size = 128 , shuffle = False , num_workers = 4 , pin_memory = True , ) model = model . cuda () model . eval () evaluator = ImageNetEvaluator ( model_name = 'ResNeXt-101-32x8d' , paper_arxiv_id = '1611.05431' ) def get_img_id ( image_name ): return image_name . split ( '/' )[ - 1 ] . replace ( '.JPEG' , '' ) with torch . no_grad (): for i , ( input , target ) in enumerate ( test_loader ): input = input . to ( device = 'cuda' , non_blocking = True ) target = target . to ( device = 'cuda' , non_blocking = True ) output = model ( input ) image_ids = [ get_img_id ( img [ 0 ]) for img in test_loader . dataset . imgs [ i * test_loader . batch_size :( i + 1 ) * test_loader . batch_size ]] evaluator . add ( dict ( zip ( image_ids , list ( output . cpu () . numpy ())))) if evaluator . cache_exists : break evaluator . save ()","title":"A full sotabench.py example"},{"location":"imagenet/#need-more-help","text":"Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"},{"location":"pascalvoc/","text":"PASCAL VOC 2012 You can view the PASCAL VOC 2012 leaderboard here . Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the VOC 2012 dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server VOC 2012 data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. Server Data Location The VOC 2012 data is located in the root of your repository on the server at .data/vision/voc2012 . In this folder is contained: VOCtrainval_11-May-2012.tar - containing validation images and annotations Your local VOC 2012 files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/voc2012' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly. How Do I Initialize an Evaluator? Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.semantic_segmentation import PASCALVOCEvaluator evaluator = PASCALVOCEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper. For example: from sotabencheval.semantic_segmentation import PASCALVOCEvaluator evaluator = PASCALVOCEvaluator ( model_name = 'PSPNet' , paper_arxiv_id = '1612.01105' ) The above will directly compare with the result of the paper when run on the server. How Do I Evaluate Predictions? The evaluator object has an .add() method to submit predictions by batch or in full. For PASCAL there are two required arguments: outputs , a 1D np.ndarray of semantic class predictions per label, and targets , a 1D np.ndarray of ground truth semantic classes per pixel. In other words, it requires flattened inputs and outputs. To elaborate, suppose you are making predictions, batch by batch, and have your model output and the original targets with batch_size 32 , and image size (520, 480) . The shape of your outputs might look like: batch_output . shape >> ( 32 , 21 , 520 , 480 ) # where 21 is the number of VOC classes batch_target . shape >> ( 32 , 520 , 480 ) We can flatten the entire output and targets to 1D vectors for each pixel: flattened_batch_output . shape >> ( 7987200 ) # flatten by taking the max class prediction # (batch_output.argmax(1).flatten() in torch with class as second dimension) flattened_batch_target . shape >> ( 7987200 ) # (batch_target.flatten() in torch) The output might look something like this: flattened_batch_output >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) flattened_batch_target >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) In both cases, the prediction and ground truth have class 6 as the semantic label for the first 5 pixels - so the model is correct. These flattened arrays can then be passed into the .add() method of the evaluator my_evaluator . update ( outputs = flattened_batch_output , targets = flattened_batch_target ) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would like something like this (for a PyTorch example): evaluator = PASCALVOCEvaluator ( model_name = 'FCN (ResNet-101)' , paper_arxiv_id = '1605.06211' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): evaluator = PASCALVOCEvaluator ( model_name = 'FCN (ResNet-101)' , paper_arxiv_id = '1605.06211' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. A full sotabench.py example Below we show an implementation for a model from the torchvision repository. This incorporates all the features explained above: (a) using the server data root, (b) using the ImageNet Evaluator, and (c) caching the evaluation logic: import PIL import torch import torchvision from torchvision.models.segmentation import fcn_resnet101 import torchvision.transforms as transforms import tqdm from sotabench_transforms import Normalize , Compose , Resize , ToTensor from sotabencheval.semantic_segmentation import PASCALVOCEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/voc2012' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' MODEL_NAME = 'fcn_resnet101' def cat_list ( images , fill_value = 0 ): max_size = tuple ( max ( s ) for s in zip ( * [ img . shape for img in images ])) batch_shape = ( len ( images ),) + max_size batched_imgs = images [ 0 ] . new ( * batch_shape ) . fill_ ( fill_value ) for img , pad_img in zip ( images , batched_imgs ): pad_img [ ... , : img . shape [ - 2 ], : img . shape [ - 1 ]] . copy_ ( img ) return batched_imgs def collate_fn ( batch ): images , targets = list ( zip ( * batch )) batched_imgs = cat_list ( images , fill_value = 0 ) batched_targets = cat_list ( targets , fill_value = 255 ) return batched_imgs , batched_targets device = torch . device ( 'cuda' ) normalize = Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) my_transforms = Compose ([ Resize (( 520 , 480 )), ToTensor (), normalize ]) dataset_test = torchvision . datasets . VOCSegmentation ( root = DATA_ROOT , year = '2012' , image_set = \"val\" , transforms = my_transforms , download = True ) test_sampler = torch . utils . data . SequentialSampler ( dataset_test ) data_loader_test = torch . utils . data . DataLoader ( dataset_test , batch_size = 32 , sampler = test_sampler , num_workers = 4 , collate_fn = collate_fn ) model = torchvision . models . segmentation . __dict__ [ 'fcn_resnet101' ]( num_classes = 21 , pretrained = True ) model . to ( device ) model . eval () evaluator = PASCALVOCEvaluator ( model_name = 'FCN (ResNet-101)' , paper_arxiv_id = '1605.06211' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) if evaluator . cache_exists : break evaluator . save () Need More Help? Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"PASCAL VOC 2012"},{"location":"pascalvoc/#pascal-voc-2012","text":"You can view the PASCAL VOC 2012 leaderboard here .","title":"PASCAL VOC 2012"},{"location":"pascalvoc/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the VOC 2012 dataset. For example, PyTorch users might use torchvision to load the dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server VOC 2012 data paths - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"pascalvoc/#server-data-location","text":"The VOC 2012 data is located in the root of your repository on the server at .data/vision/voc2012 . In this folder is contained: VOCtrainval_11-May-2012.tar - containing validation images and annotations Your local VOC 2012 files may have a different file directory structure, so you can use control flow like below to change the data path if the script is being run on sotabench servers: from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/voc2012' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' This will detect if sotabench.py is being run on the server and change behaviour accordingly.","title":"Server Data Location"},{"location":"pascalvoc/#how-do-i-initialize-an-evaluator","text":"Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.semantic_segmentation import PASCALVOCEvaluator evaluator = PASCALVOCEvaluator ( model_name = 'My Super Model' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper. For example: from sotabencheval.semantic_segmentation import PASCALVOCEvaluator evaluator = PASCALVOCEvaluator ( model_name = 'PSPNet' , paper_arxiv_id = '1612.01105' ) The above will directly compare with the result of the paper when run on the server.","title":"How Do I Initialize an Evaluator?"},{"location":"pascalvoc/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add() method to submit predictions by batch or in full. For PASCAL there are two required arguments: outputs , a 1D np.ndarray of semantic class predictions per label, and targets , a 1D np.ndarray of ground truth semantic classes per pixel. In other words, it requires flattened inputs and outputs. To elaborate, suppose you are making predictions, batch by batch, and have your model output and the original targets with batch_size 32 , and image size (520, 480) . The shape of your outputs might look like: batch_output . shape >> ( 32 , 21 , 520 , 480 ) # where 21 is the number of VOC classes batch_target . shape >> ( 32 , 520 , 480 ) We can flatten the entire output and targets to 1D vectors for each pixel: flattened_batch_output . shape >> ( 7987200 ) # flatten by taking the max class prediction # (batch_output.argmax(1).flatten() in torch with class as second dimension) flattened_batch_target . shape >> ( 7987200 ) # (batch_target.flatten() in torch) The output might look something like this: flattened_batch_output >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) flattened_batch_target >> array ([ 6 , 6 , 6 , 6 , 6 , ... ]) In both cases, the prediction and ground truth have class 6 as the semantic label for the first 5 pixels - so the model is correct. These flattened arrays can then be passed into the .add() method of the evaluator my_evaluator . update ( outputs = flattened_batch_output , targets = flattened_batch_target ) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would like something like this (for a PyTorch example): evaluator = PASCALVOCEvaluator ( model_name = 'FCN (ResNet-101)' , paper_arxiv_id = '1605.06211' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"pascalvoc/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): evaluator = PASCALVOCEvaluator ( model_name = 'FCN (ResNet-101)' , paper_arxiv_id = '1605.06211' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"pascalvoc/#a-full-sotabenchpy-example","text":"Below we show an implementation for a model from the torchvision repository. This incorporates all the features explained above: (a) using the server data root, (b) using the ImageNet Evaluator, and (c) caching the evaluation logic: import PIL import torch import torchvision from torchvision.models.segmentation import fcn_resnet101 import torchvision.transforms as transforms import tqdm from sotabench_transforms import Normalize , Compose , Resize , ToTensor from sotabencheval.semantic_segmentation import PASCALVOCEvaluator from sotabencheval.utils import is_server if is_server (): DATA_ROOT = './.data/vision/voc2012' else : # local settings DATA_ROOT = '/home/ubuntu/my_data/' MODEL_NAME = 'fcn_resnet101' def cat_list ( images , fill_value = 0 ): max_size = tuple ( max ( s ) for s in zip ( * [ img . shape for img in images ])) batch_shape = ( len ( images ),) + max_size batched_imgs = images [ 0 ] . new ( * batch_shape ) . fill_ ( fill_value ) for img , pad_img in zip ( images , batched_imgs ): pad_img [ ... , : img . shape [ - 2 ], : img . shape [ - 1 ]] . copy_ ( img ) return batched_imgs def collate_fn ( batch ): images , targets = list ( zip ( * batch )) batched_imgs = cat_list ( images , fill_value = 0 ) batched_targets = cat_list ( targets , fill_value = 255 ) return batched_imgs , batched_targets device = torch . device ( 'cuda' ) normalize = Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) my_transforms = Compose ([ Resize (( 520 , 480 )), ToTensor (), normalize ]) dataset_test = torchvision . datasets . VOCSegmentation ( root = DATA_ROOT , year = '2012' , image_set = \"val\" , transforms = my_transforms , download = True ) test_sampler = torch . utils . data . SequentialSampler ( dataset_test ) data_loader_test = torch . utils . data . DataLoader ( dataset_test , batch_size = 32 , sampler = test_sampler , num_workers = 4 , collate_fn = collate_fn ) model = torchvision . models . segmentation . __dict__ [ 'fcn_resnet101' ]( num_classes = 21 , pretrained = True ) model . to ( device ) model . eval () evaluator = PASCALVOCEvaluator ( model_name = 'FCN (ResNet-101)' , paper_arxiv_id = '1605.06211' ) with torch . no_grad (): for image , target in tqdm . tqdm ( data_loader_test ): image , target = image . to ( 'cuda' ), target . to ( 'cuda' ) output = model ( image ) output = output [ 'out' ] evaluator . add ( output . argmax ( 1 ) . flatten () . cpu () . numpy (), target . flatten () . cpu () . numpy ()) if evaluator . cache_exists : break evaluator . save ()","title":"A full sotabench.py example"},{"location":"pascalvoc/#need-more-help","text":"Head on over to the Computer Vision section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"},{"location":"squad/","text":"SQuAD You can view the SQuAD 1.1 and SQuAD 2.0 leaderboards. Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the SQuAD dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Include an Evaluation object in sotabench.py file to record the results. Point to the server SQuAD data path - popular datasets are pre-downloaded on the server. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. How Do I Initialize an Evaluator? Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.question_answering import SQuADEvaluator , SQuADVersion # for SQuAD v1.1 evaluator = SQuADEvaluator ( model_name = 'My Super Model' , version = SQuADVersion . V11 ) # for SQuAD v2.0 evaluator = SQuADEvaluator ( model_name = 'My Super Model' , version = SQuADVersion . V20 ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the SQuAD 1.1 or SQuAD 2.0 leaderboard then you will enable direct comparison with the paper's model. For example: from sotabencheval.question_answering import SQuADEvaluator , SQuADVersion evaluator = SQuADEvaluator ( model_name = 'SpanBERT' , paper_arxiv_id = '1907.10529' , version = SQuADVersion . V20 ) The above will directly compare with the result of the paper when run on the server. Server Data Location The SQuAD development data is located in the root of your repository on the server at .data/nlp/squad . In this folder is contained: dev-v1.1.json - containing SQuAD v1.1 development dataset dev-v2.0.json - containing SQuAD v2.0 development dataset You can use evaluator.dataset_path: Path to get a path to the dataset json file. In the example above it resolves to .data/nlp/squad/dev-v2.0.json on sotabench server and ./dev-v2.0.json when run locally. If you want to use a non-standard file name or location when running locally you can override the defaults like this: evaluator = SQuADEvaluator ( ... , local_root = 'mydatasets' , dataset_filename = 'data.json' ) How Do I Evaluate Predictions? The evaluator object has an .add(answers: Dict[str, str]) method to submit predictions by batch or in full. For SQuAD the expected input is a dictionary, where keys are question ids and values are text answers. For unanswerable questions the answer should be an empty string. For example: { \"57296d571d04691400779413\" : \"itself\" , \"5a89117e19b91f001a626f2d\" : \"\" } You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would look something like this (for a PyTorch example): ... evaluator = SQuADEvaluator ( model_name = 'My Super Model' , paper_arxiv_id = \"1710.10723\" , version = SQuADVersion . V11 ) with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a dict evaluator . add ( output ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. A Full sotabench.py Example Below we show an implementation for a model from the AllenNLP repository. This incorporates all the features explained above: (a) using the SQuAD Evaluator, (b) using custom dataset location when run locally, and (c) the evaluation caching logic. from sotabencheval.question_answering import SQuADEvaluator , SQuADVersion from allennlp.data import DatasetReader from allennlp.data.iterators import DataIterator from allennlp.models.archival import load_archive from allennlp.nn.util import move_to_device def load_model ( url , batch_size = 64 ): archive = load_archive ( url , cuda_device = 0 ) model = archive . model reader = DatasetReader . from_params ( archive . config [ \"dataset_reader\" ]) iterator_params = archive . config [ \"iterator\" ] iterator_params [ \"batch_size\" ] = batch_size data_iterator = DataIterator . from_params ( iterator_params ) data_iterator . index_with ( model . vocab ) return model , reader , data_iterator def evaluate ( model , dataset , data_iterator , evaluator ): model . eval () evaluator . reset_time () for batch in data_iterator ( dataset , num_epochs = 1 , shuffle = False ): batch = move_to_device ( batch , 0 ) predictions = model ( ** batch ) answers = { metadata [ 'id' ]: prediction for metadata , prediction in zip ( batch [ 'metadata' ], predictions [ 'best_span_str' ])} evaluator . add ( answers ) if evaluator . cache_exists : break evaluator = SQuADEvaluator ( local_root = \"data/nlp/squad\" , model_name = \"BiDAF (single)\" , paper_arxiv_id = \"1611.01603\" , version = SQuADVersion . V11 ) model , reader , data_iter = \\ load_model ( \"https://allennlp.s3.amazonaws.com/models/bidaf-model-2017.09.15-charpad.tar.gz\" ) dataset = reader . read ( evaluator . dataset_path ) evaluate ( model , dataset , data_iter , evaluator ) evaluator . save () print ( evaluator . results ) Need More Help? Head on over to the Natural Language Processing section of the sotabench forums if you have any questions or difficulties.","title":"SQuAD"},{"location":"squad/#squad","text":"You can view the SQuAD 1.1 and SQuAD 2.0 leaderboards.","title":"SQuAD"},{"location":"squad/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the SQuAD dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Include an Evaluation object in sotabench.py file to record the results. Point to the server SQuAD data path - popular datasets are pre-downloaded on the server. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"squad/#how-do-i-initialize-an-evaluator","text":"Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.question_answering import SQuADEvaluator , SQuADVersion # for SQuAD v1.1 evaluator = SQuADEvaluator ( model_name = 'My Super Model' , version = SQuADVersion . V11 ) # for SQuAD v2.0 evaluator = SQuADEvaluator ( model_name = 'My Super Model' , version = SQuADVersion . V20 ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the SQuAD 1.1 or SQuAD 2.0 leaderboard then you will enable direct comparison with the paper's model. For example: from sotabencheval.question_answering import SQuADEvaluator , SQuADVersion evaluator = SQuADEvaluator ( model_name = 'SpanBERT' , paper_arxiv_id = '1907.10529' , version = SQuADVersion . V20 ) The above will directly compare with the result of the paper when run on the server.","title":"How Do I Initialize an Evaluator?"},{"location":"squad/#server-data-location","text":"The SQuAD development data is located in the root of your repository on the server at .data/nlp/squad . In this folder is contained: dev-v1.1.json - containing SQuAD v1.1 development dataset dev-v2.0.json - containing SQuAD v2.0 development dataset You can use evaluator.dataset_path: Path to get a path to the dataset json file. In the example above it resolves to .data/nlp/squad/dev-v2.0.json on sotabench server and ./dev-v2.0.json when run locally. If you want to use a non-standard file name or location when running locally you can override the defaults like this: evaluator = SQuADEvaluator ( ... , local_root = 'mydatasets' , dataset_filename = 'data.json' )","title":"Server Data Location"},{"location":"squad/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add(answers: Dict[str, str]) method to submit predictions by batch or in full. For SQuAD the expected input is a dictionary, where keys are question ids and values are text answers. For unanswerable questions the answer should be an empty string. For example: { \"57296d571d04691400779413\" : \"itself\" , \"5a89117e19b91f001a626f2d\" : \"\" } You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would look something like this (for a PyTorch example): ... evaluator = SQuADEvaluator ( model_name = 'My Super Model' , paper_arxiv_id = \"1710.10723\" , version = SQuADVersion . V11 ) with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a dict evaluator . add ( output ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"squad/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"squad/#a-full-sotabenchpy-example","text":"Below we show an implementation for a model from the AllenNLP repository. This incorporates all the features explained above: (a) using the SQuAD Evaluator, (b) using custom dataset location when run locally, and (c) the evaluation caching logic. from sotabencheval.question_answering import SQuADEvaluator , SQuADVersion from allennlp.data import DatasetReader from allennlp.data.iterators import DataIterator from allennlp.models.archival import load_archive from allennlp.nn.util import move_to_device def load_model ( url , batch_size = 64 ): archive = load_archive ( url , cuda_device = 0 ) model = archive . model reader = DatasetReader . from_params ( archive . config [ \"dataset_reader\" ]) iterator_params = archive . config [ \"iterator\" ] iterator_params [ \"batch_size\" ] = batch_size data_iterator = DataIterator . from_params ( iterator_params ) data_iterator . index_with ( model . vocab ) return model , reader , data_iterator def evaluate ( model , dataset , data_iterator , evaluator ): model . eval () evaluator . reset_time () for batch in data_iterator ( dataset , num_epochs = 1 , shuffle = False ): batch = move_to_device ( batch , 0 ) predictions = model ( ** batch ) answers = { metadata [ 'id' ]: prediction for metadata , prediction in zip ( batch [ 'metadata' ], predictions [ 'best_span_str' ])} evaluator . add ( answers ) if evaluator . cache_exists : break evaluator = SQuADEvaluator ( local_root = \"data/nlp/squad\" , model_name = \"BiDAF (single)\" , paper_arxiv_id = \"1611.01603\" , version = SQuADVersion . V11 ) model , reader , data_iter = \\ load_model ( \"https://allennlp.s3.amazonaws.com/models/bidaf-model-2017.09.15-charpad.tar.gz\" ) dataset = reader . read ( evaluator . dataset_path ) evaluate ( model , dataset , data_iter , evaluator ) evaluator . save () print ( evaluator . results )","title":"A Full sotabench.py Example"},{"location":"squad/#need-more-help","text":"Head on over to the Natural Language Processing section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"},{"location":"wikitext103/","text":"WikiText-103 You can view the WikiText-103 leaderboard here . Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get language model predictions on the WikiText-103 dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server WikiText-103 data path - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. Server Data Location The WikiText-103 development data is located in the root of your repository on the server at .data/nlp/wikitext-103/wikitext-103-v1.zip . The archive contains a folder wikitext-103 with the following files: wiki.train.tokens wiki.valid.tokens wiki.test.tokens It is the original zip file released here . We are running the benchmark on the wiki.test.tokens dataset. We have two helper methods that will unpack the dataset for you and give you the pathlib.Path to the test file. The first option test_set_path is available once you instantiate the WikiText103Evaluator : ... evaluator = WikiText103Evaluator ( model_name = \"Transformer-XL Large\" , paper_arxiv_id = \"1901.02860\" , paper_pwc_id = \"transformer-xl-attentive-language-models\" , local_root = '/content/wikitext-103' ) # dataset_path is pathlib.Path and points to wikitext.test.tokens with evaluator . test_set_path . open () as f : test_data = torch . tensor ( tokenizer . encode ( f . read ())) . to ( \"cuda\" ) There is a second option available if you are evaluating multiple models and need to use the same dataset multiple times - WikiText103Evaluator.get_test_set_path(local_root) . This will get the path before you initialize a WikiText evaluator: from sotabencheval.language_modelling import WikiText103Evaluator test_file_path = WikiText103Evaluator . get_test_set_path ( '/home/ubuntu/my_data/wiki103' ) with test_file_path . open () as f : content = f . read () How Do I Initialize an Evaluator? Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.language_modelling import WikiText103Evaluator evaluator = WikiText103Evaluator ( model_name = 'Model name as found in paperswithcode website' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the Wikitext-103 leaderboard then you will enable direct comparison with the paper's model. If the arxiv_id is not available you can use paperswithcode.com id. Below is an example of an evaluator that matches Transformer XL : from sotabencheval.language_modelling import WikiText103Evaluator evaluator = WikiText103Evaluator ( model_name = \"Transformer-XL Large\" , paper_arxiv_id = \"1901.02860\" , paper_pwc_id = \"transformer-xl-attentive-language-models\" , local_root = \"path_to_your_data\" , ) The above will directly compare with the result of the paper when run on the server. How Do I Evaluate Predictions? The evaluator object has an .add(log_probs, targets) method to submit predictions by batch or in full. We expect you to give us the log probability of a batch of target tokens and the target tokens themselves. The log_probs can be either: a 0d \"tensor\" ( np.ndarray / torch.tensor ) - summed log probability of all targets tokens a 2d \"tensor\" ( np.ndarray / torch.tensor ) - log probabilities of each target token, the log_probs.shape should match targets.shape a 3d \"tensor\" ( np.ndarray / torch.tensor ) - distribution of log probabilities for each position in the sequence, we will gather the probabilities of target tokens for you. It is recommended to use third or second option as it allows us to check your perplexity calculations. If your model uses subword tokenization you don't need convert subwords to full words. You are free to report probability of each subword: we will adjust the perplexity normalization accordingly. Just make sure to set subword_tokenization=True in your evaluator. Here is an example of how to report results (for a PyTorch example): evaluator = WikiText103Evaluator ( model_name = 'GPT-2 Small' , paper_pwc_id = \"language-models-are-unsupervised-multitask\" , local_root = \"path_to_your_data\" , subword_tokenization = True ) # run you data preprocessing, in case of GPT-2 the preprocessing removes moses artifacts with torch . no_grad (): model . eval () for input , target in data_loader : output = model ( input ) log_probs = torch . LogSoftmax ( output , dim =- 1 ) target_log_probs = output . gather ( - 1 , targets . unsqueeze ( - 1 )) evaluator . add ( target_log_probs , target ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for input , target in data_loader : # ... output = model ( input ) log_probs = #... evaluator . add ( log_probs , target ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. A full sotabench.py example Below we show an implementation for a model from the huggingface/transformers . This incorporates all the features explained above: (a) using the server data, (b) using the WikiText-103 Evaluator, and (c) caching the evaluation logic: import torch from tqdm import tqdm from sotabencheval.language_modelling import WikiText103Evaluator model = torch . hub . load ( 'huggingface/transformers' , 'modelWithLMHead' , 'transfo-xl-wt103' ) . to ( \"cuda\" ) tokenizer = torch . hub . load ( 'huggingface/transformers' , 'tokenizer' , 'transfo-xl-wt103' ) evaluator = WikiText103Evaluator ( model_name = \"Transformer-XL Large\" , paper_arxiv_id = \"1901.02860\" , paper_pwc_id = \"transformer-xl-attentive-language-models\" , local_root = '/content/wikitext-103' ) with evaluator . test_set_path . open () as f : test_data = torch . tensor ( tokenizer . encode ( f . read ())) seq_len = 128 with torch . no_grad (): evaluator . reset_timer () model . eval () X , Y , mems = test_data [ None , : - 1 ], test_data [ None , 1 :], None for s in tqdm ( range ( 0 , X . shape [ - 1 ], seq_len )): x , y = X [ ... , s : s + seq_len ] . to ( \"cuda\" ), Y [ ... , s : s + seq_len ] . to ( \"cuda\" ) log_probs , mems , * _ = model ( input_ids = x , mems = mems ) evaluator . add ( log_probs , y ) if evaluator . cache_exists : break evaluator . save () evaluator . print_results () You can run this example on Google Colab . Need More Help? Head on over to the Natural Language Processing section of the sotabench forums if you have any questions or difficulties.","title":"WikiText-103"},{"location":"wikitext103/#wikitext-103","text":"You can view the WikiText-103 leaderboard here .","title":"WikiText-103"},{"location":"wikitext103/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get language model predictions on the WikiText-103 dataset. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Point to the server WikiText-103 data path - popular datasets are pre-downloaded on the server. Include an Evaluation object in sotabench.py file to record the results. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"wikitext103/#server-data-location","text":"The WikiText-103 development data is located in the root of your repository on the server at .data/nlp/wikitext-103/wikitext-103-v1.zip . The archive contains a folder wikitext-103 with the following files: wiki.train.tokens wiki.valid.tokens wiki.test.tokens It is the original zip file released here . We are running the benchmark on the wiki.test.tokens dataset. We have two helper methods that will unpack the dataset for you and give you the pathlib.Path to the test file. The first option test_set_path is available once you instantiate the WikiText103Evaluator : ... evaluator = WikiText103Evaluator ( model_name = \"Transformer-XL Large\" , paper_arxiv_id = \"1901.02860\" , paper_pwc_id = \"transformer-xl-attentive-language-models\" , local_root = '/content/wikitext-103' ) # dataset_path is pathlib.Path and points to wikitext.test.tokens with evaluator . test_set_path . open () as f : test_data = torch . tensor ( tokenizer . encode ( f . read ())) . to ( \"cuda\" ) There is a second option available if you are evaluating multiple models and need to use the same dataset multiple times - WikiText103Evaluator.get_test_set_path(local_root) . This will get the path before you initialize a WikiText evaluator: from sotabencheval.language_modelling import WikiText103Evaluator test_file_path = WikiText103Evaluator . get_test_set_path ( '/home/ubuntu/my_data/wiki103' ) with test_file_path . open () as f : content = f . read ()","title":"Server Data Location"},{"location":"wikitext103/#how-do-i-initialize-an-evaluator","text":"Add this to your code - before you start batching over the dataset and making predictions: from sotabencheval.language_modelling import WikiText103Evaluator evaluator = WikiText103Evaluator ( model_name = 'Model name as found in paperswithcode website' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the Wikitext-103 leaderboard then you will enable direct comparison with the paper's model. If the arxiv_id is not available you can use paperswithcode.com id. Below is an example of an evaluator that matches Transformer XL : from sotabencheval.language_modelling import WikiText103Evaluator evaluator = WikiText103Evaluator ( model_name = \"Transformer-XL Large\" , paper_arxiv_id = \"1901.02860\" , paper_pwc_id = \"transformer-xl-attentive-language-models\" , local_root = \"path_to_your_data\" , ) The above will directly compare with the result of the paper when run on the server.","title":"How Do I Initialize an Evaluator?"},{"location":"wikitext103/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add(log_probs, targets) method to submit predictions by batch or in full. We expect you to give us the log probability of a batch of target tokens and the target tokens themselves. The log_probs can be either: a 0d \"tensor\" ( np.ndarray / torch.tensor ) - summed log probability of all targets tokens a 2d \"tensor\" ( np.ndarray / torch.tensor ) - log probabilities of each target token, the log_probs.shape should match targets.shape a 3d \"tensor\" ( np.ndarray / torch.tensor ) - distribution of log probabilities for each position in the sequence, we will gather the probabilities of target tokens for you. It is recommended to use third or second option as it allows us to check your perplexity calculations. If your model uses subword tokenization you don't need convert subwords to full words. You are free to report probability of each subword: we will adjust the perplexity normalization accordingly. Just make sure to set subword_tokenization=True in your evaluator. Here is an example of how to report results (for a PyTorch example): evaluator = WikiText103Evaluator ( model_name = 'GPT-2 Small' , paper_pwc_id = \"language-models-are-unsupervised-multitask\" , local_root = \"path_to_your_data\" , subword_tokenization = True ) # run you data preprocessing, in case of GPT-2 the preprocessing removes moses artifacts with torch . no_grad (): model . eval () for input , target in data_loader : output = model ( input ) log_probs = torch . LogSoftmax ( output , dim =- 1 ) target_log_probs = output . gather ( - 1 , targets . unsqueeze ( - 1 )) evaluator . add ( target_log_probs , target ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"wikitext103/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for input , target in data_loader : # ... output = model ( input ) log_probs = #... evaluator . add ( log_probs , target ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"wikitext103/#a-full-sotabenchpy-example","text":"Below we show an implementation for a model from the huggingface/transformers . This incorporates all the features explained above: (a) using the server data, (b) using the WikiText-103 Evaluator, and (c) caching the evaluation logic: import torch from tqdm import tqdm from sotabencheval.language_modelling import WikiText103Evaluator model = torch . hub . load ( 'huggingface/transformers' , 'modelWithLMHead' , 'transfo-xl-wt103' ) . to ( \"cuda\" ) tokenizer = torch . hub . load ( 'huggingface/transformers' , 'tokenizer' , 'transfo-xl-wt103' ) evaluator = WikiText103Evaluator ( model_name = \"Transformer-XL Large\" , paper_arxiv_id = \"1901.02860\" , paper_pwc_id = \"transformer-xl-attentive-language-models\" , local_root = '/content/wikitext-103' ) with evaluator . test_set_path . open () as f : test_data = torch . tensor ( tokenizer . encode ( f . read ())) seq_len = 128 with torch . no_grad (): evaluator . reset_timer () model . eval () X , Y , mems = test_data [ None , : - 1 ], test_data [ None , 1 :], None for s in tqdm ( range ( 0 , X . shape [ - 1 ], seq_len )): x , y = X [ ... , s : s + seq_len ] . to ( \"cuda\" ), Y [ ... , s : s + seq_len ] . to ( \"cuda\" ) log_probs , mems , * _ = model ( input_ids = x , mems = mems ) evaluator . add ( log_probs , y ) if evaluator . cache_exists : break evaluator . save () evaluator . print_results () You can run this example on Google Colab .","title":"A full sotabench.py example"},{"location":"wikitext103/#need-more-help","text":"Head on over to the Natural Language Processing section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"},{"location":"wmt/","text":"WMT You can view the WMT Machine Translation leaderboards: WMT2014 English-German WMT2014 English-French WMT2019 English-German Getting Started You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the WMT datasets. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Include an Evaluation object in sotabench.py file to record the results. Point to the server WMT data path - popular datasets are pre-downloaded on the server. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below. How Do I Initialize an Evaluator? Before you start batching over the dataset and making predictions you need to create an evaluator instance to record results for a given leaderboard. For example, to evaluate on WMT2014 News English-French test set add this to your code: from sotabencheval.machine_translation import WMTEvaluator , WMTDataset , Language evaluator = WMTEvaluator ( dataset = WMTDataset . News2014 , source_lang = Language . English , target_lang = Language . French , local_root = 'mydatasets' , model_name = 'My Super Model' ) You can use evaluator.source_dataset_path: Path and evaluator.target_dataset_path: Path to get paths to the source and target SGML files. In the example above the first one resolves to .data/nlp/wmt/newstest2014-fren-src.en.sgm on sotabench server and mydatasets/newstest2014-fren-src.en.sgm when run locally. If you want to use non-standard file names locally you can override the defaults like this: evaluator = WMTEvaluator ( ... , local_root = 'mydatasets' source_dataset_filename = 'english.sgm' , target_dataset_filename = 'french.sgm' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper's model. For example: evaluator = WMTEvaluator ( dataset = WMTDataset . News2019 , source_lang = Language . English , target_lang = Language . German , local_root = \"mydatasets\" , model_name = \"Facebook-FAIR (single)\" , paper_arxiv_id = \"1907.06616\" ) The above will directly compare with the result of the paper when run on the server. By default the evaluator computes a detokenized mixed-case SacreBLEU score. To get a tokenized BLEU score as well, during construction of the evaluator set a tokenization: Callable[[str], str] parameter to a function that tokenizes an input segment and returns segment with tokens separated by space, f.e.: def get_tokenization (): mt = sacremoses . MosesTokenizer () def tokenize ( sentence ): return mt . tokenize ( sentence , return_str = True ) return tokenize evaluator = WMTEvaluator ( ... , tokenization = get_tokenization () ) Instead of parsing the dataset files by yourself you can access raw segments as strings: for segment_id , text in evaluator . source_segments : # translate text # or get segments within document context for document in evaluator . source_documents : context = [ segment . text for segment in document . segments ] for segment in document . segments : segment_id , text = segment . id , segment . text # translate text in context How Do I Evaluate Predictions? The evaluator object has an .add(answers: Dict[str, str]) method to submit predictions by batch or in full. For WMT the expected input is a dictionary, where keys are source segments ids and values are translated segments (segment id is created by concatenating document id and the original segment id, separted by # .) For example: evaluator . add ({ 'bbc.381790#1' : 'Waliser AMs sorgen sich um \"Aussehen wie Muppets\"' , 'bbc.381790#2' : 'Unter einigen AMs herrscht Best\u00fcrzung \u00fcber einen...' , 'bbc.381790#3' : 'Sie ist aufgrund von Pl\u00e4nen entstanden, den Namen...' }) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would look something like this (for a PyTorch example): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a dict evaluator . add ( output ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database. How Do I Cache Evaluation? Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly. A Full sotabench.py Example Below we show an implementation for a model from the torchhub repository. This incorporates all the features explained above: (a) using the WMT Evaluator, (b) accessing segments from evaluator, and (c) the evaluation caching logic. For clarity we omit batching and simply translate segment by segment. from sotabencheval.machine_translation import WMTEvaluator , WMTDataset , Language from tqdm import tqdm import torch evaluator = WMTEvaluator ( dataset = WMTDataset . News2019 , source_lang = Language . English , target_lang = Language . German , local_root = \"data/nlp/wmt\" , model_name = \"Facebook-FAIR (single)\" , paper_arxiv_id = \"1907.06616\" ) model = torch . hub . load ( 'pytorch/fairseq' , 'transformer.wmt19.en-de.single_model' , force_reload = True , tokenizer = 'moses' , bpe = 'fastbpe' ) . cuda () for sid , text in tqdm ( evaluator . source_segments . items ()): translated = model . translate ( text ) evaluator . add ({ sid : translated }) if evaluator . cache_exists : break evaluator . save () print ( evaluator . results ) Need More Help? Head on over to the Natural Language Processing section of the sotabench forums if you have any questions or difficulties.","title":"WMT"},{"location":"wmt/#wmt","text":"You can view the WMT Machine Translation leaderboards: WMT2014 English-German WMT2014 English-French WMT2019 English-German","title":"WMT"},{"location":"wmt/#getting-started","text":"You'll need the following in the root of your repository: sotabench.py file - contains benchmarking logic; the server will run this on each commit requirements.txt file - Python dependencies to be installed before running sotabench.py sotabench_setup.sh (optional) - any advanced dependencies or setup, e.g. compilation You can write whatever you want in your sotabench.py file to get model predictions on the WMT datasets. But you will need to record your results for the server, and you'll want to avoid doing things like downloading the dataset on the server. So you should: Include an Evaluation object in sotabench.py file to record the results. Point to the server WMT data path - popular datasets are pre-downloaded on the server. Use Caching (optional) - to speed up evaluation by hashing the first batch of predictions. We explain how to do these various steps below.","title":"Getting Started"},{"location":"wmt/#how-do-i-initialize-an-evaluator","text":"Before you start batching over the dataset and making predictions you need to create an evaluator instance to record results for a given leaderboard. For example, to evaluate on WMT2014 News English-French test set add this to your code: from sotabencheval.machine_translation import WMTEvaluator , WMTDataset , Language evaluator = WMTEvaluator ( dataset = WMTDataset . News2014 , source_lang = Language . English , target_lang = Language . French , local_root = 'mydatasets' , model_name = 'My Super Model' ) You can use evaluator.source_dataset_path: Path and evaluator.target_dataset_path: Path to get paths to the source and target SGML files. In the example above the first one resolves to .data/nlp/wmt/newstest2014-fren-src.en.sgm on sotabench server and mydatasets/newstest2014-fren-src.en.sgm when run locally. If you want to use non-standard file names locally you can override the defaults like this: evaluator = WMTEvaluator ( ... , local_root = 'mydatasets' source_dataset_filename = 'english.sgm' , target_dataset_filename = 'french.sgm' ) If you are reproducing a model from a paper, then you can enter the arXiv ID. If you put in the same model name string as on the leaderboard then you will enable direct comparison with the paper's model. For example: evaluator = WMTEvaluator ( dataset = WMTDataset . News2019 , source_lang = Language . English , target_lang = Language . German , local_root = \"mydatasets\" , model_name = \"Facebook-FAIR (single)\" , paper_arxiv_id = \"1907.06616\" ) The above will directly compare with the result of the paper when run on the server. By default the evaluator computes a detokenized mixed-case SacreBLEU score. To get a tokenized BLEU score as well, during construction of the evaluator set a tokenization: Callable[[str], str] parameter to a function that tokenizes an input segment and returns segment with tokens separated by space, f.e.: def get_tokenization (): mt = sacremoses . MosesTokenizer () def tokenize ( sentence ): return mt . tokenize ( sentence , return_str = True ) return tokenize evaluator = WMTEvaluator ( ... , tokenization = get_tokenization () ) Instead of parsing the dataset files by yourself you can access raw segments as strings: for segment_id , text in evaluator . source_segments : # translate text # or get segments within document context for document in evaluator . source_documents : context = [ segment . text for segment in document . segments ] for segment in document . segments : segment_id , text = segment . id , segment . text # translate text in context","title":"How Do I Initialize an Evaluator?"},{"location":"wmt/#how-do-i-evaluate-predictions","text":"The evaluator object has an .add(answers: Dict[str, str]) method to submit predictions by batch or in full. For WMT the expected input is a dictionary, where keys are source segments ids and values are translated segments (segment id is created by concatenating document id and the original segment id, separted by # .) For example: evaluator . add ({ 'bbc.381790#1' : 'Waliser AMs sorgen sich um \"Aussehen wie Muppets\"' , 'bbc.381790#2' : 'Unter einigen AMs herrscht Best\u00fcrzung \u00fcber einen...' , 'bbc.381790#3' : 'Sie ist aufgrund von Pl\u00e4nen entstanden, den Namen...' }) You can do this all at once in a single call to add() , but more naturally, you will probably loop over the dataset and call the method for the outputs of each batch. That would look something like this (for a PyTorch example): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a dict evaluator . add ( output ) When you are done, you can get the results locally by running: evaluator . get_results () But for the server you want to save the results by running: evaluator . save () This method serialises the results and model metadata and stores to the server database.","title":"How Do I Evaluate Predictions?"},{"location":"wmt/#how-do-i-cache-evaluation","text":"Sotabench reruns your script on every commit. This is good because it acts like continuous integration in checking for bugs and changes, but can be annoying if the model hasn't changed and evaluation is lengthy. Fortunately sotabencheval has caching logic that you can use. The idea is that after the first batch, we hash the model outputs and the current metrics and this tells us if the model is the same given the dataset. You can include hashing within an evaluation loop like follows (in the following example for a PyTorch repository): with torch . no_grad (): for i , ( input , target ) in enumerate ( data_loader ): ... output = model ( input ) # potentially formatting of the output here to be a list of dicts evaluator . add ( output ) if evaluator . cache_exists : break evaluator . save () If the hash is the same as in the server, we infer that the model hasn't changed, so we simply return hashed results rather than running the whole evaluation again. Caching is very useful if you have large models, or a repository that is evaluating multiple models, as it speeds up evaluation significantly.","title":"How Do I Cache Evaluation?"},{"location":"wmt/#a-full-sotabenchpy-example","text":"Below we show an implementation for a model from the torchhub repository. This incorporates all the features explained above: (a) using the WMT Evaluator, (b) accessing segments from evaluator, and (c) the evaluation caching logic. For clarity we omit batching and simply translate segment by segment. from sotabencheval.machine_translation import WMTEvaluator , WMTDataset , Language from tqdm import tqdm import torch evaluator = WMTEvaluator ( dataset = WMTDataset . News2019 , source_lang = Language . English , target_lang = Language . German , local_root = \"data/nlp/wmt\" , model_name = \"Facebook-FAIR (single)\" , paper_arxiv_id = \"1907.06616\" ) model = torch . hub . load ( 'pytorch/fairseq' , 'transformer.wmt19.en-de.single_model' , force_reload = True , tokenizer = 'moses' , bpe = 'fastbpe' ) . cuda () for sid , text in tqdm ( evaluator . source_segments . items ()): translated = model . translate ( text ) evaluator . add ({ sid : translated }) if evaluator . cache_exists : break evaluator . save () print ( evaluator . results )","title":"A Full sotabench.py Example"},{"location":"wmt/#need-more-help","text":"Head on over to the Natural Language Processing section of the sotabench forums if you have any questions or difficulties.","title":"Need More Help?"}]}